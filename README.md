# Optimization-Algorithms
in this repository, you will find some optimization algorithms I implemented.

## Like:
- Batch Gradient Descent
- Mini-Batch Gradient Descent
- Stochastic Gradient Descent
- Momentum-based Gradient Descent
- Nesterov Accelerated  Gradient Descent (NAG)
- Adagrad
- RMSProp
- Adam